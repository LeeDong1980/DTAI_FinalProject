{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArtistName</th>\n",
       "      <th>SongName</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>release_date</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>valence</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>popularity</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>explicit</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>key</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>mode</th>\n",
       "      <th>liveness</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The Knife</td>\n",
       "      <td>Epochs</td>\n",
       "      <td>pop</td>\n",
       "      <td>An intersection of the plain\\nby the bank of\\n...</td>\n",
       "      <td>2010-02-01</td>\n",
       "      <td>0.399000</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-13.893</td>\n",
       "      <td>128.751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>343189.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.527000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Ledisi</td>\n",
       "      <td>Us 4Ever</td>\n",
       "      <td>urban</td>\n",
       "      <td>This here is real, you know what? Love ain't u...</td>\n",
       "      <td>2017-09-22</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.3060</td>\n",
       "      <td>0.0799</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.523</td>\n",
       "      <td>-5.174</td>\n",
       "      <td>80.027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>269067.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0929</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Maddie Poppe</td>\n",
       "      <td>Not Losing You</td>\n",
       "      <td>pop</td>\n",
       "      <td>You don't know, if you belong here\\nYou're afr...</td>\n",
       "      <td>2019-05-17</td>\n",
       "      <td>0.847000</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-5.092</td>\n",
       "      <td>139.445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>226469.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3290</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Brenda Lee</td>\n",
       "      <td>Just A Closer Walk With Thee</td>\n",
       "      <td>pop</td>\n",
       "      <td>I am weak but Thou art strong Jesus keep me fr...</td>\n",
       "      <td>2005-03-29</td>\n",
       "      <td>0.486000</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.2010</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.300</td>\n",
       "      <td>-13.731</td>\n",
       "      <td>82.654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>242293.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Mudhoney</td>\n",
       "      <td>Next Time</td>\n",
       "      <td>rock</td>\n",
       "      <td>I've been thinking of you\\nYeah, I've been thi...</td>\n",
       "      <td>2008-05-20</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.7150</td>\n",
       "      <td>0.0669</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.840</td>\n",
       "      <td>-6.448</td>\n",
       "      <td>191.797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>181653.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2530</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41042</td>\n",
       "      <td>Samantha Jade</td>\n",
       "      <td>Silent Night</td>\n",
       "      <td>pop</td>\n",
       "      <td>Silent night, holy night\\nAll is calm, all is ...</td>\n",
       "      <td>2018-11-02</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.2900</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-11.451</td>\n",
       "      <td>176.195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>196960.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0966</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41043</td>\n",
       "      <td>Miki Howard</td>\n",
       "      <td>Three Wishes</td>\n",
       "      <td>pop</td>\n",
       "      <td>(Three wishes just three simple wishes)\\nIf I ...</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>0.331000</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.3320</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-7.523</td>\n",
       "      <td>79.913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285074.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41044</td>\n",
       "      <td>Allister</td>\n",
       "      <td>Camouflage</td>\n",
       "      <td>rock</td>\n",
       "      <td>Hiding in back and never make a sound\\nIf you ...</td>\n",
       "      <td>2002-10-08</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.8530</td>\n",
       "      <td>0.0507</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.909</td>\n",
       "      <td>-5.534</td>\n",
       "      <td>132.695</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133720.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41045</td>\n",
       "      <td>YFN Lucci</td>\n",
       "      <td>Propane</td>\n",
       "      <td>urban</td>\n",
       "      <td>[YFN Lucci:]\\nSomethin' like propane, uh, too ...</td>\n",
       "      <td>2018-02-16</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.638</td>\n",
       "      <td>-7.535</td>\n",
       "      <td>142.051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>258392.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3010</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41046</td>\n",
       "      <td>Richie Kotzen</td>\n",
       "      <td>I Would</td>\n",
       "      <td>rock</td>\n",
       "      <td>I feel like a prisoner in my own life\\nand I'm...</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>0.0348</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-10.130</td>\n",
       "      <td>119.493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>213267.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41047 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ArtistName                      SongName  Genre  \\\n",
       "0          The Knife                        Epochs    pop   \n",
       "1             Ledisi                      Us 4Ever  urban   \n",
       "2       Maddie Poppe                Not Losing You    pop   \n",
       "3         Brenda Lee  Just A Closer Walk With Thee    pop   \n",
       "4           Mudhoney                     Next Time   rock   \n",
       "...              ...                           ...    ...   \n",
       "41042  Samantha Jade                  Silent Night    pop   \n",
       "41043    Miki Howard                  Three Wishes    pop   \n",
       "41044       Allister                    Camouflage   rock   \n",
       "41045      YFN Lucci                       Propane  urban   \n",
       "41046  Richie Kotzen                       I Would   rock   \n",
       "\n",
       "                                                  Lyrics release_date  \\\n",
       "0      An intersection of the plain\\nby the bank of\\n...   2010-02-01   \n",
       "1      This here is real, you know what? Love ain't u...   2017-09-22   \n",
       "2      You don't know, if you belong here\\nYou're afr...   2019-05-17   \n",
       "3      I am weak but Thou art strong Jesus keep me fr...   2005-03-29   \n",
       "4      I've been thinking of you\\nYeah, I've been thi...   2008-05-20   \n",
       "...                                                  ...          ...   \n",
       "41042  Silent night, holy night\\nAll is calm, all is ...   2018-11-02   \n",
       "41043  (Three wishes just three simple wishes)\\nIf I ...   2001-01-01   \n",
       "41044  Hiding in back and never make a sound\\nIf you ...   2002-10-08   \n",
       "41045  [YFN Lucci:]\\nSomethin' like propane, uh, too ...   2018-02-16   \n",
       "41046  I feel like a prisoner in my own life\\nand I'm...   2003-01-01   \n",
       "\n",
       "       acousticness  danceability  valence  speechiness  popularity  energy  \\\n",
       "0          0.399000         0.215   0.0342       0.0386        11.0   0.215   \n",
       "1          0.148000         0.682   0.3060       0.0799        43.0   0.523   \n",
       "2          0.847000         0.424   0.3250       0.0328        51.0   0.289   \n",
       "3          0.486000         0.504   0.2010       0.0287        35.0   0.300   \n",
       "4          0.000032         0.164   0.7150       0.0669         5.0   0.840   \n",
       "...             ...           ...      ...          ...         ...     ...   \n",
       "41042      0.454000         0.154   0.2900       0.0303        17.0   0.381   \n",
       "41043      0.331000         0.640   0.3320       0.0297         5.0   0.481   \n",
       "41044      0.000112         0.503   0.8530       0.0507        12.0   0.909   \n",
       "41045      0.102000         0.765   0.2920       0.2140        35.0   0.638   \n",
       "41046      0.691000         0.643   0.2320       0.0348        17.0   0.196   \n",
       "\n",
       "       loudness    tempo  explicit  duration_ms   key  instrumentalness  mode  \\\n",
       "0       -13.893  128.751       0.0     343189.0   9.0          0.527000   1.0   \n",
       "1        -5.174   80.027       0.0     269067.0   4.0          0.000000   0.0   \n",
       "2        -5.092  139.445       0.0     226469.0   0.0          0.000000   1.0   \n",
       "3       -13.731   82.654       0.0     242293.0   2.0          0.000013   1.0   \n",
       "4        -6.448  191.797       0.0     181653.0   5.0          0.178000   0.0   \n",
       "...         ...      ...       ...          ...   ...               ...   ...   \n",
       "41042   -11.451  176.195       0.0     196960.0   7.0          0.000011   1.0   \n",
       "41043    -7.523   79.913       0.0     285074.0   1.0          0.000000   1.0   \n",
       "41044    -5.534  132.695       0.0     133720.0  11.0          0.000000   1.0   \n",
       "41045    -7.535  142.051       1.0     258392.0   4.0          0.000000   0.0   \n",
       "41046   -10.130  119.493       0.0     213267.0   5.0          0.000000   1.0   \n",
       "\n",
       "       liveness  time_signature  \n",
       "0        0.1070             3.0  \n",
       "1        0.0929             4.0  \n",
       "2        0.3290             4.0  \n",
       "3        0.1060             4.0  \n",
       "4        0.2530             4.0  \n",
       "...         ...             ...  \n",
       "41042    0.0966             3.0  \n",
       "41043    0.0988             4.0  \n",
       "41044    0.0947             4.0  \n",
       "41045    0.3010             4.0  \n",
       "41046    0.1100             4.0  \n",
       "\n",
       "[41047 rows x 20 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw = pd.read_csv(\"TrainingData_raw.csv\")\n",
    "data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArtistName</th>\n",
       "      <th>SongName</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>valence</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>energy</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>popularity</th>\n",
       "      <th>loudness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The Knife</td>\n",
       "      <td>Epochs</td>\n",
       "      <td>pop</td>\n",
       "      <td>intersection plain bank great stream animal ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Ledisi</td>\n",
       "      <td>Us 4Ever</td>\n",
       "      <td>urban</td>\n",
       "      <td>real know love unconditional give work choose ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Maddie Poppe</td>\n",
       "      <td>Not Losing You</td>\n",
       "      <td>pop</td>\n",
       "      <td>know belong afraid stay long lay arm dear tell...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Brenda Lee</td>\n",
       "      <td>Just A Closer Walk With Thee</td>\n",
       "      <td>pop</td>\n",
       "      <td>weak thou art strong jesus keep wrong satisfie...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Mudhoney</td>\n",
       "      <td>Next Time</td>\n",
       "      <td>rock</td>\n",
       "      <td>think yeah think tell ya gon na next time get ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37230</td>\n",
       "      <td>Samantha Jade</td>\n",
       "      <td>Silent Night</td>\n",
       "      <td>pop</td>\n",
       "      <td>silent night holy night calm bright round yon ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37231</td>\n",
       "      <td>Miki Howard</td>\n",
       "      <td>Three Wishes</td>\n",
       "      <td>pop</td>\n",
       "      <td>three wish three simple wish three wish would ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37232</td>\n",
       "      <td>Allister</td>\n",
       "      <td>Camouflage</td>\n",
       "      <td>rock</td>\n",
       "      <td>hide back never make sound speak thought turn ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37233</td>\n",
       "      <td>YFN Lucci</td>\n",
       "      <td>Propane</td>\n",
       "      <td>urban</td>\n",
       "      <td>somethin like propane uh much smoke man hey ye...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37234</td>\n",
       "      <td>Richie Kotzen</td>\n",
       "      <td>I Would</td>\n",
       "      <td>rock</td>\n",
       "      <td>feel like prisoner life try break away place d...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37235 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ArtistName                      SongName  Genre  \\\n",
       "0          The Knife                        Epochs    pop   \n",
       "1             Ledisi                      Us 4Ever  urban   \n",
       "2       Maddie Poppe                Not Losing You    pop   \n",
       "3         Brenda Lee  Just A Closer Walk With Thee    pop   \n",
       "4           Mudhoney                     Next Time   rock   \n",
       "...              ...                           ...    ...   \n",
       "37230  Samantha Jade                  Silent Night    pop   \n",
       "37231    Miki Howard                  Three Wishes    pop   \n",
       "37232       Allister                    Camouflage   rock   \n",
       "37233      YFN Lucci                       Propane  urban   \n",
       "37234  Richie Kotzen                       I Would   rock   \n",
       "\n",
       "                                                  Lyrics  acousticness  \\\n",
       "0      intersection plain bank great stream animal ca...             0   \n",
       "1      real know love unconditional give work choose ...             0   \n",
       "2      know belong afraid stay long lay arm dear tell...             1   \n",
       "3      weak thou art strong jesus keep wrong satisfie...             0   \n",
       "4      think yeah think tell ya gon na next time get ...             0   \n",
       "...                                                  ...           ...   \n",
       "37230  silent night holy night calm bright round yon ...             0   \n",
       "37231  three wish three simple wish three wish would ...             0   \n",
       "37232  hide back never make sound speak thought turn ...             0   \n",
       "37233  somethin like propane uh much smoke man hey ye...             0   \n",
       "37234  feel like prisoner life try break away place d...             1   \n",
       "\n",
       "       danceability  valence  subjectivity  energy  speechiness  popularity  \\\n",
       "0                 0        0             1       0            0           0   \n",
       "1                 1        0             1       1            0           0   \n",
       "2                 0        0             1       0            0           1   \n",
       "3                 1        0             1       0            0           0   \n",
       "4                 0        1             0       1            0           0   \n",
       "...             ...      ...           ...     ...          ...         ...   \n",
       "37230             0        0             0       0            0           0   \n",
       "37231             1        0             0       0            0           0   \n",
       "37232             1        1             0       1            0           0   \n",
       "37233             1        0             0       1            0           0   \n",
       "37234             1        0             0       0            0           0   \n",
       "\n",
       "       loudness  tempo  explicit  \n",
       "0             0      1         0  \n",
       "1             1      0         0  \n",
       "2             1      1         0  \n",
       "3             0      0         0  \n",
       "4             1      1         0  \n",
       "...         ...    ...       ...  \n",
       "37230         0      1         0  \n",
       "37231         1      0         0  \n",
       "37232         1      1         0  \n",
       "37233         1      1         1  \n",
       "37234         0      0         0  \n",
       "\n",
       "[37235 rows x 14 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = pd.read_csv(\"TrainingData_clean.csv\")\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArtistName</th>\n",
       "      <th>SongName</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>valence</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>energy</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>popularity</th>\n",
       "      <th>loudness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The Knife</td>\n",
       "      <td>Epochs</td>\n",
       "      <td>pop</td>\n",
       "      <td>intersection plain bank great stream animal ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Ledisi</td>\n",
       "      <td>Us 4Ever</td>\n",
       "      <td>urban</td>\n",
       "      <td>real know love unconditional give work choose ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Maddie Poppe</td>\n",
       "      <td>Not Losing You</td>\n",
       "      <td>pop</td>\n",
       "      <td>know belong afraid stay long lay arm dear tell...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Brenda Lee</td>\n",
       "      <td>Just A Closer Walk With Thee</td>\n",
       "      <td>pop</td>\n",
       "      <td>weak thou art strong jesus keep wrong satisfie...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Mudhoney</td>\n",
       "      <td>Next Time</td>\n",
       "      <td>rock</td>\n",
       "      <td>think yeah think tell ya gon na next time get ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ArtistName                      SongName  Genre  \\\n",
       "0     The Knife                        Epochs    pop   \n",
       "1        Ledisi                      Us 4Ever  urban   \n",
       "2  Maddie Poppe                Not Losing You    pop   \n",
       "3    Brenda Lee  Just A Closer Walk With Thee    pop   \n",
       "4      Mudhoney                     Next Time   rock   \n",
       "\n",
       "                                              Lyrics  acousticness  \\\n",
       "0  intersection plain bank great stream animal ca...             0   \n",
       "1  real know love unconditional give work choose ...             0   \n",
       "2  know belong afraid stay long lay arm dear tell...             1   \n",
       "3  weak thou art strong jesus keep wrong satisfie...             0   \n",
       "4  think yeah think tell ya gon na next time get ...             0   \n",
       "\n",
       "   danceability  valence  subjectivity  energy  speechiness  popularity  \\\n",
       "0             0        0             1       0            0           0   \n",
       "1             1        0             1       1            0           0   \n",
       "2             0        0             1       0            0           1   \n",
       "3             1        0             1       0            0           0   \n",
       "4             0        1             0       1            0           0   \n",
       "\n",
       "   loudness  tempo  explicit  \n",
       "0         0      1         0  \n",
       "1         1      0         0  \n",
       "2         1      1         0  \n",
       "3         0      0         0  \n",
       "4         1      1         0  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_train_df = pd.read_csv(\"TrainingData_clean.csv\")\n",
    "w2v_test_df = pd.read_csv(\"TrainingData_clean.csv\")\n",
    "w2v_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vecTrain = w2v_train_df['Lyrics'][0:]\n",
    "w2v_x_train = w2v_train_df['Lyrics'][0:25000]\n",
    "w2v_y_train = w2v_train_df[['acousticness','danceability','valence','subjectivity','energy','speechiness','popularity','loudness','tempo','explicit']][0:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [intersection, plain, bank, great, stream, ani...\n",
       "1        [real, know, love, unconditional, give, work, ...\n",
       "2        [know, belong, afraid, stay, long, lay, arm, d...\n",
       "3        [weak, thou, art, strong, jesus, keep, wrong, ...\n",
       "4        [think, yeah, think, tell, ya, gon, na, next, ...\n",
       "                               ...                        \n",
       "24995    [oh, sound, love, oh, sound, love, wan, na, ge...\n",
       "24996    [wake, eye, open, wide, mercy, wait, thats, br...\n",
       "24997    [yeah, yeah, look, like, look, like, check, se...\n",
       "24998    [hard, happy, home, town, hard, happy, sun, go...\n",
       "24999    [tonight, night, come, cold, wait, long, long,...\n",
       "Name: Lyrics, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_x_train = x_train.str.split()\n",
    "w2v_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 1, 0, ..., 1, 0, 0],\n",
       "       [1, 0, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 1, ..., 1, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_y_train = w2v_y_train.to_numpy()\n",
    "w2v_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['intersection', 'plain', 'bank', 'great', 'stream', 'animal', 'carcass', 'skeleton', 'would', 'entomb', 'step', 'form', 'terrace', 'succession', 'embed', 'year', 'accumulate', 'tranquilly', 'small', 'quadruped', 'perfectly', 'epochs', 'collect']),\n",
       "       list(['real', 'know', 'love', 'unconditional', 'give', 'work', 'choose', 'say', 'hold', 'love', 'worth', 'find', 'reason', 'stay', 'way', 'would', 'love', 'fight', 'argue', 'make', 'right', 'leave', 'go', 'back', 'together', 'love', 'strong', 'right', 'wrong', 'build', 'break', 'guess', 'u', '4ever', 'tear', 'blow', 'ups', 'every', 'day', 'something', 'back', 'forth', 'know', 'know', 'perfect', 'oh', 'crazy', 'amaze', 'every', 'time', 'still', 'find', 'reason', 'want', 'stay', 'maybe', 'love', 'fight', 'argue', 'make', 'right', 'leave', 'go', 'back', 'together', 'love', 'strong', 'right', 'wrong', 'build', 'break', 'guess', 'u', '4ever', 'know', 'know', 'meant', 'ever', 'think', 'get', 'go', 'everything', 'make', 'know', 'know', 'realize', 'always', 'mine', 'cause', 'love', 'want', 'love', 'need', 'always', 'gon', 'na', 'say', 'crazy', 'never', 'lie', 'cause', 'lord', 'know', 'perfect', 'building', 'strong', 'pray', 'never', 'leave', 'alone', 'cause', 'baby', 'love', 'fight', 'argue', 'make', 'right', 'leave', 'go', 'back', 'together', 'love', 'strong', 'right', 'wrong', 'build', 'break', 'guess', 'u', '4ever']),\n",
       "       list(['know', 'belong', 'afraid', 'stay', 'long', 'lay', 'arm', 'dear', 'tell', 'get', 'wrong', 'boy', 'tell', 'ooh', 'ooh', 'ooh', 'ooh', 'ooh', 'world', 'spinnin', 'ooh', 'ooh', 'ooh', 'ooh', 'ooh', 'stop', 'second', 'cause', 'dream', 'seem', 'know', 'could', 'hurt', 'hold', 'hand', 'come', 'true', 'cause', 'could', 'get', 'hold', 'head', 'high', 'everything', 'desert', 'lose', 'get', 'scar', 'make', 'doubt', 'know', 'leave', 'know', 'hard', 'give', 'everything', 'everything', 'know', 'still', 'boy', 'tell', 'ooh', 'ooh', 'ooh', 'ooh', 'ooh', 'world', 'spinnin', 'ooh', 'ooh', 'ooh', 'ooh', 'ooh', 'stop', 'second', 'cause', 'dream', 'seem', 'know', 'could', 'hurt', 'hold', 'hand', 'come', 'true', 'cause', 'could', 'get', 'hold', 'head', 'high', 'everything', 'desert', 'lose', 'say', 'honey', 'tell', 'really', 'gon', 'na', 'last', 'forever', 'like', 'others', 'love', 'say', 'even', 'though', 'mood', 'change', 'like', 'weather', 'still', 'love', 'love', 'cause', 'dream', 'seem', 'know', 'could', 'hurt', 'hold', 'hand', 'come', 'true', 'cause', 'could', 'get', 'hold', 'head', 'high', 'everything', 'desert', 'lose', 'lose', 'lose', 'know', 'belong', 'know', 'get', 'wrong']),\n",
       "       ...,\n",
       "       list(['hide', 'back', 'never', 'make', 'sound', 'speak', 'thought', 'turn', 'around', 'lead', 'target', 'youth', 'find', 'courage', 'leave', 'shell', 'yet', 'find', 'short', 'term', 'courage', 'flask', 'another', 'waste', 'day', 'never', 'get', 'back', 'lead', 'target', 'youth', 'would', 'talk', 'certain', 'find', 'courage', 'leave', 'shell', 'yet', 'torn', 'apart', 'camouflage', 'background', 'hold', 'thing', 'deep', 'never', 'show', 'true', 'self', 'kid', 'meet', 'find', 'shade', 'imagination', 'one', 'day', 'forget', 'world', 'hide', 'back', 'never', 'make', 'sound', 'speak', 'thought', 'turn', 'around', 'lead', 'target', 'youth', 'would', 'talk', 'certain', 'find', 'courage', 'leave', 'shell', 'yet', 'torn', 'apart', 'camouflage', 'background', 'hold', 'thing', 'deep', 'never', 'show', 'true', 'self', 'kid', 'meet', 'find', 'shade', 'imagination', 'one', 'day', 'forget', 'world', 'one', 'day', 'forget', 'world', 'find', 'short', 'term', 'courage', 'flask', 'another', 'waste', 'day', 'never', 'get', 'back']),\n",
       "       list(['somethin', 'like', 'propane', 'uh', 'much', 'smoke', 'man', 'hey', 'yeah', 'get', 'feeling', 'hoe', 'man', 'yeah', 'call', 'snowman', 'froze', 'chain', 'yeah', 'aye', 'matchin', 'bling', 'bling', 'whole', 'gang', 'workin', 'like', 'goat', 'grindin', 'need', 'hope', 'yeah', 'shine', 'get', 'one', 'hand', 'uh', 'gon', 'hold', 'yeah', 'whoever', 'want', 'smoke', 'yeah', 'one', 'hand', 'glock', 'yeah', 'know', 'go', 'uh', 'one', 'hand', 'man', 'pray', 'pray', 'play', 'gon', 'play', 'gon', 'may', 'make', 'uh', 'naw', 'make', 'make', 'call', 'ball', 'basketball', 'international', 'flyer', 'asteroid', 'like', 'rack', 'get', 'wacked', 'dawg', 'shootin', 'nigga', 'family', 'house', 'get', 'man', 'involve', 'make', 'get', 'man', 'nah', 'get', 'pay', 'saw', 'knock', 'ya', 'head', 'coupe', 'head', 'one', 'opps', 'block', 'seat', 'low', 'get', 'chop', 'cock', 'cut', 'one', 'car', 'get', 'lookin', 'like', 'chop', 'shop', 'hell', 'nah', 'stop', 'every', 'night', 'pray', 'drop', 'every', 'night', 'pray', 'lookin', 'smoke', 'somethin', 'like', 'propane', 'uh', 'much', 'smoke', 'man', 'hey', 'yeah', 'get', 'feeling', 'hoe', 'man', 'yeah', 'call', 'snowman', 'froze', 'chain', 'yeah', 'aye', 'matchin', 'bling', 'bling', 'whole', 'gang', 'workin', 'like', 'goat', 'grindin', 'need', 'hope', 'yeah', 'shine', 'get', 'one', 'hand', 'uh', 'gon', 'hold', 'yeah', 'whoever', 'want', 'smoke', 'yeah', 'one', 'hand', 'glock', 'yeah', 'know', 'go', 'money', 'power', 'respect', 'one', 'hand', 'grippin', 'tec', 'whole', 'lotta', 'water', 'drippin', 'neck', 'vvs', 'whole', 'lotta', 'lean', 'drippin', 'cup', 'cvs', 'whole', 'lotta', 'pound', 'drop', 'first', 'ups', 'nigga', 'want', 'famous', 'leave', 'fuck', 'nigga', 'plankin', 'brain', 'floor', 'see', 'thinkin', 'nut', 'nut', 'glock', 'trust', 'wait', 'bust', 'leave', 'pussy', 'nigga', 'stick', 'put', 'pistol', 'ship', 'two', 'cup', 'twistin', 'finger', 'whole', 'lotta', 'new', 'buck', 'knock', 'em', 'pick', 'em', 'bout', 'action', 'talk', 'leave', 'scene', 'cloudy', 'lotta', 'smoke', 'around', 'somethin', 'like', 'propane', 'uh', 'much', 'smoke', 'man', 'hey', 'yeah', 'get', 'feeling', 'hoe', 'man', 'yeah', 'call', 'snowman', 'froze', 'chain', 'yeah', 'aye', 'matchin', 'bling', 'bling', 'whole', 'gang', 'workin', 'like', 'goat', 'grindin', 'need', 'hope', 'yeah', 'shine', 'get', 'one', 'hand', 'uh', 'gon', 'hold', 'yeah', 'whoever', 'want', 'smoke', 'yeah', 'one', 'hand', 'glock', 'yeah', 'know', 'go', 'move', 'bag', 'like', 'garage', 'truck', 'huh', 'get', 'order', 'use', 'ride', 'marta', 'bus', 'huh', 'charter', 'u', 'nigga', 'hard', 'enough', 'huh', 'want', 'war', 'u', 'huh', 'startin', 'u', 'nigga', 'part', 'u', 'grindin', 'lately', 'grindin', 'car', 'slide', 'havin', 'trouble', 'decidin', 'whole', 'lotta', 'diamond', 'face', 'see', 'time', 'even', 'sign', 'deal', 'probably', 'swipin', 'still', 'haha', 'hand', 'huh', 'rubberband', 'huh', 'probably', 'much', 'huh', 'rockin', 'much', 'huh', 'one', 'call', 'get', 'touch', 'dj', 'screw', 'cup', 'yeah', 'whole', 'gang', 'do', 'blow', 'somethin', 'like', 'propane', 'uh', 'much', 'smoke', 'man', 'hey', 'yeah', 'get', 'feeling', 'hoe', 'man', 'yeah', 'call', 'snowman', 'froze', 'chain', 'yeah', 'aye', 'matchin', 'bling', 'bling', 'whole', 'gang', 'workin', 'like', 'goat', 'grindin', 'need', 'hope', 'yeah', 'shine', 'get', 'one', 'hand', 'uh', 'gon', 'hold', 'yeah', 'whoever', 'want', 'smoke', 'yeah', 'one', 'hand', 'glock', 'yeah', 'know', 'go']),\n",
       "       list(['feel', 'like', 'prisoner', 'life', 'try', 'break', 'away', 'place', 'darker', 'use', 'harder', 'everyday', 'could', 'turn', 'around', 'would', 'get', 'foot', 'back', 'ground', 'would', 'would', 'find', 'mean', 'satisfy', 'would', 'would', 'search', 'soul', 'need', 'find', 'peace', 'mind', 'would', 'would', 'try', 'hard', 'believe', 'high', 'power', 'easier', 'let', 'day', 'seem', 'long', 'time', 'still', 'rush', 'wan', 'na', 'almost', 'stand', 'enough', 'see', 'maybe', 'lose', 'could', 'turn', 'around', 'would', 'get', 'foot', 'back', 'ground', 'would', 'would', 'find', 'mean', 'satisfy', 'would', 'would', 'search', 'soul', 'need', 'find', 'peace', 'mind', 'would', 'would', 'try', 'hard', 'believe', 'high', 'power', 'easier', 'let'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vecTrain = word2vecTrain.str.split()\n",
    "word2vecTrain = word2vecTrain.to_numpy()\n",
    "word2vecTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(word2vecTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看看詞語的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(w2v_model, words, topn=10):\n",
    "    similar_df = pd.DataFrame()\n",
    "    for word in words:\n",
    "        try:\n",
    "            similar_words = pd.DataFrame(w2v_model.wv.most_similar(word, topn=topn), columns=[word, 'cos'])\n",
    "            similar_df = pd.concat([similar_df, similar_words], axis=1)\n",
    "        except:\n",
    "            print(word, \"not found in Word2Vec model!\")\n",
    "    return similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good not found in Word2Vec model!\n",
      "see not found in Word2Vec model!\n",
      "control not found in Word2Vec model!\n",
      "late not found in Word2Vec model!\n",
      "weight not found in Word2Vec model!\n",
      "roll not found in Word2Vec model!\n",
      "wheel not found in Word2Vec model!\n",
      "sun not found in Word2Vec model!\n",
      "happy not found in Word2Vec model!\n",
      "walk not found in Word2Vec model!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(model,[\"good\", \"see\", \"control\", \"late\", \"weight\", \"roll\", \"wheel\", \"sun\", \"happy\", \"walk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"TrainingData_clean.csv\")\n",
    "x_train_raw = train_df['Lyrics'][0:25000]\n",
    "y_train_raw = train_df[['acousticness','danceability','valence','subjectivity','energy','speechiness','popularity','loudness','tempo','explicit']][0:25000]\n",
    "x_test_raw = train_df['Lyrics'][25000:]\n",
    "y_test_raw = train_df[['acousticness','danceability','valence','subjectivity','energy','speechiness','popularity','loudness','tempo','explicit']][25000:]\n",
    "w2v_model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        intersection plain bank great stream animal ca...\n",
       "1        real know love unconditional give work choose ...\n",
       "2        know belong afraid stay long lay arm dear tell...\n",
       "3        weak thou art strong jesus keep wrong satisfie...\n",
       "4        think yeah think tell ya gon na next time get ...\n",
       "                               ...                        \n",
       "24995    oh sound love oh sound love wan na get away wa...\n",
       "24996    wake eye open wide mercy wait thats brand new ...\n",
       "24997    yeah yeah look like look like check see homey ...\n",
       "24998    hard happy home town hard happy sun go hard ha...\n",
       "24999    tonight night come cold wait long long time ge...\n",
       "Name: Lyrics, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_raw.str.split()\n",
    "x_test = x_test_raw.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [intersection, plain, bank, great, stream, ani...\n",
       "1        [real, know, love, unconditional, give, work, ...\n",
       "2        [know, belong, afraid, stay, long, lay, arm, d...\n",
       "3        [weak, thou, art, strong, jesus, keep, wrong, ...\n",
       "4        [think, yeah, think, tell, ya, gon, na, next, ...\n",
       "                               ...                        \n",
       "24995    [oh, sound, love, oh, sound, love, wan, na, ge...\n",
       "24996    [wake, eye, open, wide, mercy, wait, thats, br...\n",
       "24997    [yeah, yeah, look, like, look, like, check, se...\n",
       "24998    [hard, happy, home, town, hard, happy, sun, go...\n",
       "24999    [tonight, night, come, cold, wait, long, long,...\n",
       "Name: Lyrics, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.to_numpy()\n",
    "y_train = y_train_raw.to_numpy()\n",
    "\n",
    "x_test = x_test.to_numpy()\n",
    "y_test = y_test_raw.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [intersection, plain, bank, great, stream, ani...\n",
       "1        [real, know, love, unconditional, give, work, ...\n",
       "2        [know, belong, afraid, stay, long, lay, arm, d...\n",
       "3        [weak, thou, art, strong, jesus, keep, wrong, ...\n",
       "4        [think, yeah, think, tell, ya, gon, na, next, ...\n",
       "                               ...                        \n",
       "24995    [oh, sound, love, oh, sound, love, wan, na, ge...\n",
       "24996    [wake, eye, open, wide, mercy, wait, thats, br...\n",
       "24997    [yeah, yeah, look, like, look, like, check, se...\n",
       "24998    [hard, happy, home, town, hard, happy, sun, go...\n",
       "24999    [tonight, night, come, cold, wait, long, long,...\n",
       "Name: Lyrics, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(w2v_model.wv.vocab.items()) + 1, w2v_model.vector_size))\n",
    "word2idx = {}\n",
    "\n",
    "vocab_list = [(word, w2v_model.wv[word]) for word, _ in w2v_model.wv.vocab.items()]\n",
    "for i, vocab in enumerate(vocab_list):\n",
    "    word, vec = vocab\n",
    "    embedding_matrix[i + 1] = vec\n",
    "    word2idx[word] = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                            output_dim=embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_index(corpus):\n",
    "    new_corpus = []\n",
    "    for doc in corpus:\n",
    "        new_doc = []\n",
    "        for word in doc:\n",
    "            try:\n",
    "                new_doc.append(word2idx[word])\n",
    "            except:\n",
    "                new_doc.append(0)\n",
    "        new_corpus.append(new_doc)\n",
    "    return np.array(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (25000, 100)\n",
      "Sample: [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   1   2   3   4   5   6   7   8   9  10  11  12  13\n",
      "   14  15  16  17   0  18   0  19   0  20]\n",
      " [ 46  47  48  49  50  51  52  53  54  55  41  56  22  22  57  58  59  60\n",
      "   53  61  62  31  32  63  33  64  23  35  36  37  38  39  40  41  42  23\n",
      "   43  38  44  45  46  47  48  49  22  22  65  66  67  68  40  69  37  22\n",
      "   22  70  71  72  73  23  63  23  74  71  75  76  28  59  77  78  73  79\n",
      "   22  57  80  43  81  77  39  82  73  83  23  35  36  37  38  39  40  41\n",
      "   42  23  43  38  44  45  46  47  48  49]\n",
      " [104 105  69 106 107  68 108  37 109  22  39  22 110  25  69  69  22  62\n",
      "   91  90  92  92  92  92  92  93  94  92  92  92  92  92  95  96  73  97\n",
      "   98  22  99 100  29 101 102 103  73  99  68  29 104 105  69 106 107  28\n",
      "  111  90 112  75  76 113 114 115 116  23  28 117 118 119 120 115 121  62\n",
      "   23  23  73  97  98  22  99 100  29 101 102 103  73  99  68  29 104 105\n",
      "   69 106 107 107 107  22  84  22  68  44]]\n"
     ]
    }
   ],
   "source": [
    "PADDING_LENGTH = 100\n",
    "x_train = text_to_index(x_train)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=PADDING_LENGTH)\n",
    "print(\"Shape:\", x_train.shape)\n",
    "print(\"Sample:\", x_train[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = text_to_index(x_test)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=PADDING_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = sequence.pad_sequences(x_train, maxlen=100)\n",
    "#x_test = sequence.pad_sequences(x_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([104, 105,  69, 106, 107,  68, 108,  37, 109,  22,  39,  22, 110,\n",
       "        25,  69,  69,  22,  62,  91,  90,  92,  92,  92,  92,  92,  93,\n",
       "        94,  92,  92,  92,  92,  92,  95,  96,  73,  97,  98,  22,  99,\n",
       "       100,  29, 101, 102, 103,  73,  99,  68,  29, 104, 105,  69, 106,\n",
       "       107,  28, 111,  90, 112,  75,  76, 113, 114, 115, 116,  23,  28,\n",
       "       117, 118, 119, 120, 115, 121,  62,  23,  23,  73,  97,  98,  22,\n",
       "        99, 100,  29, 101, 102, 103,  73,  99,  68,  29, 104, 105,  69,\n",
       "       106, 107, 107, 107,  22,  84,  22,  68,  44])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建置RNN神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "#model.add(Embedding(10000, 10))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(17, activation='relu'))\n",
    "model.add(Dense(23, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(16))    \n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax')) \n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, None, 100)         2155900   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 10)                4440      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 17)                187       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 23)                414       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                240       \n",
      "=================================================================\n",
      "Total params: 2,161,181\n",
      "Trainable params: 5,281\n",
      "Non-trainable params: 2,155,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/20\n",
      "22500/22500 [==============================] - 4s 200us/step - loss: 0.5993 - accuracy: 0.6872 - val_loss: 0.5327 - val_accuracy: 0.7166\n",
      "Epoch 2/20\n",
      "22500/22500 [==============================] - 4s 196us/step - loss: 0.5285 - accuracy: 0.7137 - val_loss: 0.5153 - val_accuracy: 0.7222\n",
      "Epoch 3/20\n",
      "22500/22500 [==============================] - 4s 196us/step - loss: 0.5147 - accuracy: 0.7164 - val_loss: 0.5029 - val_accuracy: 0.7229\n",
      "Epoch 4/20\n",
      "22500/22500 [==============================] - 4s 196us/step - loss: 0.5059 - accuracy: 0.7197 - val_loss: 0.4971 - val_accuracy: 0.7255\n",
      "Epoch 5/20\n",
      "22500/22500 [==============================] - 4s 197us/step - loss: 0.5013 - accuracy: 0.7227 - val_loss: 0.4949 - val_accuracy: 0.7304\n",
      "Epoch 6/20\n",
      "22500/22500 [==============================] - 5s 201us/step - loss: 0.4984 - accuracy: 0.7259 - val_loss: 0.4946 - val_accuracy: 0.7326\n",
      "Epoch 7/20\n",
      "22500/22500 [==============================] - 5s 202us/step - loss: 0.4963 - accuracy: 0.7275 - val_loss: 0.4934 - val_accuracy: 0.7320\n",
      "Epoch 8/20\n",
      "22500/22500 [==============================] - 5s 208us/step - loss: 0.4949 - accuracy: 0.7280 - val_loss: 0.4917 - val_accuracy: 0.7319\n",
      "Epoch 9/20\n",
      "22500/22500 [==============================] - 5s 201us/step - loss: 0.4932 - accuracy: 0.7298 - val_loss: 0.4912 - val_accuracy: 0.7310\n",
      "Epoch 10/20\n",
      "22500/22500 [==============================] - 4s 193us/step - loss: 0.4906 - accuracy: 0.7327 - val_loss: 0.4885 - val_accuracy: 0.7352\n",
      "Epoch 11/20\n",
      "22500/22500 [==============================] - 5s 203us/step - loss: 0.4881 - accuracy: 0.7348 - val_loss: 0.4865 - val_accuracy: 0.7389\n",
      "Epoch 12/20\n",
      "22500/22500 [==============================] - 5s 213us/step - loss: 0.4851 - accuracy: 0.7392 - val_loss: 0.4837 - val_accuracy: 0.7423\n",
      "Epoch 13/20\n",
      "22500/22500 [==============================] - 5s 206us/step - loss: 0.4825 - accuracy: 0.7422 - val_loss: 0.4831 - val_accuracy: 0.7408\n",
      "Epoch 14/20\n",
      "22500/22500 [==============================] - 5s 205us/step - loss: 0.4807 - accuracy: 0.7446 - val_loss: 0.4815 - val_accuracy: 0.7442\n",
      "Epoch 15/20\n",
      "22500/22500 [==============================] - 5s 201us/step - loss: 0.4790 - accuracy: 0.7464 - val_loss: 0.4806 - val_accuracy: 0.7463\n",
      "Epoch 16/20\n",
      "22500/22500 [==============================] - 4s 199us/step - loss: 0.4773 - accuracy: 0.7483 - val_loss: 0.4792 - val_accuracy: 0.7472\n",
      "Epoch 17/20\n",
      "22500/22500 [==============================] - 4s 198us/step - loss: 0.4760 - accuracy: 0.7495 - val_loss: 0.4792 - val_accuracy: 0.7477\n",
      "Epoch 18/20\n",
      "22500/22500 [==============================] - 5s 205us/step - loss: 0.4747 - accuracy: 0.7512 - val_loss: 0.4783 - val_accuracy: 0.7461\n",
      "Epoch 19/20\n",
      "22500/22500 [==============================] - 4s 198us/step - loss: 0.4731 - accuracy: 0.7529 - val_loss: 0.4765 - val_accuracy: 0.7479\n",
      "Epoch 20/20\n",
      "22500/22500 [==============================] - 5s 201us/step - loss: 0.4717 - accuracy: 0.7540 - val_loss: 0.4753 - val_accuracy: 0.7519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1eed767e7c8>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=300, epochs=20, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12235/12235 [==============================] - 3s 234us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "測試資料的 loss: 0.47823\n",
      "測試資料的正確率: 0.7487536668777466\n"
     ]
    }
   ],
   "source": [
    "print(f'測試資料的 loss: {score[0]:.5f}')\n",
    "print(f'測試資料的正確率: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "predictX = model.predict_classes(x_test)\n",
    "predictY = model.predict_classes(y_test)\n",
    "print(predictX[0], predictY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 143,  143,  143,  143,  143,  143,  143,   74,  112,   74,  219,\n",
       "       1340,  635,   68, 1340,  219, 1340,  997,   68, 1340,  219, 1340,\n",
       "         25,   25,   25, 1340,  219, 1340,  102,  997,   68,   38, 2044,\n",
       "       1340,  506,  122,  506,  262,   68, 1340,  117,  262,  349,   22,\n",
       "        362,  544,   68, 1340,  170,   68, 1340,  170,  170,  209,   68,\n",
       "       1340,  916,  917,  916,  917,  916,  917, 1340,  219, 1340,  635,\n",
       "         68, 1340,  219, 1340,   74,   74,  319, 2044, 1340,  219, 1340,\n",
       "         28,  219, 1340,  319,  170,  209, 2196,  219, 1340,   68,   40,\n",
       "        219,   68,  219, 1340,  997,   68, 1340,  219, 1340,  635,   68,\n",
       "       1340])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "pX_test = x_test\n",
    "pX_test = pad_sequences(pX_test, maxlen=PADDING_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 143,  143,  143,  143,  143,  143,  143,   74,  112,   74,  219,\n",
       "       1340,  635,   68, 1340,  219, 1340,  997,   68, 1340,  219, 1340,\n",
       "         25,   25,   25, 1340,  219, 1340,  102,  997,   68,   38, 2044,\n",
       "       1340,  506,  122,  506,  262,   68, 1340,  117,  262,  349,   22,\n",
       "        362,  544,   68, 1340,  170,   68, 1340,  170,  170,  209,   68,\n",
       "       1340,  916,  917,  916,  917,  916,  917, 1340,  219, 1340,  635,\n",
       "         68, 1340,  219, 1340,   74,   74,  319, 2044, 1340,  219, 1340,\n",
       "         28,  219, 1340,  319,  170,  209, 2196,  219, 1340,   68,   40,\n",
       "        219,   68,  219, 1340,  997,   68, 1340,  219, 1340,  635,   68,\n",
       "       1340])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pX_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "pX_test = x_test\n",
    "pX_test = pad_sequences(pX_test, maxlen=PADDING_LENGTH)\n",
    "Y_preds = model.predict(pX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (12235, 10)\n",
      "X_Sample: [ 143  143  143  143  143  143  143   74  112   74  219 1340  635   68\n",
      " 1340  219 1340  997   68 1340  219 1340   25   25   25 1340  219 1340\n",
      "  102  997   68   38 2044 1340  506  122  506  262   68 1340  117  262\n",
      "  349   22  362  544   68 1340  170   68 1340  170  170  209   68 1340\n",
      "  916  917  916  917  916  917 1340  219 1340  635   68 1340  219 1340\n",
      "   74   74  319 2044 1340  219 1340   28  219 1340  319  170  209 2196\n",
      "  219 1340   68   40  219   68  219 1340  997   68 1340  219 1340  635\n",
      "   68 1340]\n",
      "Y_Sample: [0.1625539  0.8613092  0.70206505 0.6903693  0.78143054 0.03723553\n",
      " 0.14775255 0.7992332  0.4015279  0.07936683]\n",
      "Shape: (12235, 10)\n",
      "X_Sample: [  58   58 1369  646 1339   38 1369  646 1339   38 1369  646 1339   38\n",
      " 1369  646   58   58 1369  646   23   38 1369  646   23   38 1369  646\n",
      "   23   38 1369  646   58   58 1369  646  771   38 1369  646  406   38\n",
      " 1369  646  406   38 1369  646   58   58 1369  646   23   38 1369  646\n",
      "   23   38 1369  646   23   38 1369  646   58   58 1369  646  771   38\n",
      " 1369  646  771   38 1369  646  771   38 1369  646   58   58 1369  646\n",
      "  406   38 1369  646  406   38 1369  646  406   38 1369  646   58   58\n",
      " 1369  646]\n",
      "Y_Sample: [0.21659994 0.5575535  0.3622264  0.7663418  0.73516464 0.01152971\n",
      " 0.07915619 0.7691153  0.4616468  0.03128079]\n",
      "Shape: (12235, 10)\n",
      "X_Sample: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0  1783    93    12 19136   380   140    22   472\n",
      "    54     9   102   403  1369  2698  2999  1328    12   456    93   487\n",
      "    23    25    25   380   316    25    25   380   316    25    25   380\n",
      "   316   788   759    53  3382  1077    40  6164  1268    25    25   380\n",
      "   316   575     0   380   244  1083  5281  2547   981  5633  1077   764\n",
      "  1993   125  1227   364  3988    23    99   771   114   727  3988    23\n",
      "    99   771   114   727]\n",
      "Y_Sample: [0.27712023 0.4989628  0.31490028 0.7575034  0.6404375  0.00698996\n",
      " 0.07011878 0.71098363 0.46181342 0.01216003]\n",
      "Shape: (12235, 10)\n",
      "X_Sample: [ 2404  9509  1070  3363   115  4428  2507  1048  3282   539   291   860\n",
      "   777   458    37  3034  2554   182   478  3493    25   126   539  1160\n",
      "   471   114   474   702    77   889 12143  1027  1070   286   753  1977\n",
      "  4460  3363  1087    77    22   166  2478    38   589  2404   154  1799\n",
      "  2404  3458  2404  1579  1734    99   542   257  2404   153   900  2404\n",
      "   694   729    77   112    23   118  3416   319   727  1284   185   172\n",
      "   990   257    90 11242   847    37  1436    63   293   359    74  8736\n",
      "   117    28   702  2404  9509  1070  3363   115  4428  2507  1048  3282\n",
      "   539   291   860   777]\n",
      "Y_Sample: [0.21610641 0.5410185  0.46902496 0.285797   0.7589896  0.00912723\n",
      " 0.11605734 0.8044445  0.620373   0.0137769 ]\n",
      "Shape: (12235, 10)\n",
      "X_Sample: [  101 13437   703  5033  5231 20951    99  1678    67 20951  1678 15866\n",
      "    67   564  2419 13228   104   555    40   637   103  3555    73  2609\n",
      "  5465  6615  1072  3912  1584  3899   761   380  3555    58   678 20951\n",
      "  1404  1732  4100   154   511  2610     0   154 20951    99  1678    67\n",
      " 20951  1678 15866    67   511 12782   883  1614   511  2492  1678    41\n",
      "   836  6843 16454  1283   399 12966   692    99   836 19716  7620  1678\n",
      "  2298     0    58  4306 20951  1404  1732  4100    73   184     0   154\n",
      " 20951    99  1678    67 20951  1678 15866    67  4678    99  1678    67\n",
      "    58    25   511 20951]\n",
      "Y_Sample: [0.13246545 0.41835293 0.26058757 0.47638977 0.82216537 0.01014885\n",
      " 0.06500101 0.8602309  0.5513902  0.04285678]\n",
      "Shape: (12235, 10)\n",
      "X_Sample: [ 601   68  167 3418 1360   39   83  309  490  129  259   22  172  608\n",
      "   68   68 4374  938   28 3547   68 1063 6004   22   23 1029 3246   23\n",
      "  212 2425  576 1692   68 1205   83  309  853   22 2490   22  319   58\n",
      " 2680   99   90  853  219  661 2324   37  102 2060  575  105  864  828\n",
      "   83   39  601   22 4465  997 1591 1494 1494 1494   83   25  167  316\n",
      "   73   22  112  151   76   33   83   25  167  316 2680 2680 2680   83\n",
      "   25  167  316   73   22  112  151   76   33   83   25  167  316 2680\n",
      " 2680 2680]\n",
      "Y_Sample: [0.08167896 0.7547495  0.4340111  0.6536973  0.8802247  0.0794189\n",
      " 0.11591902 0.88840246 0.47717118 0.6090689 ]\n",
      "Shape: (12235, 10)\n",
      "X_Sample: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0 1092\n",
      " 4399  703]\n",
      "Y_Sample: [0.3914302  0.26234928 0.14619562 0.4139161  0.47464624 0.00734842\n",
      " 0.03307369 0.5575053  0.47595057 0.01649654]\n",
      "Shape: (12235, 10)\n",
      "X_Sample: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0 1494 2289\n",
      "  317  537  154  280 3930 2161  259 2075 1494  357  490  126 3597  257\n",
      "  297   23 5929  340  764  659 3594  313  253 3942   71 5497   72 1494\n",
      "  490  194  194 1079 8521 1494  490 1361  167   58   28 2820 3594  313\n",
      " 6480  569  764 1494  490  194  194 1079 8521 1494  490 1361 1494 1494\n",
      "  490 1361  490 1361  167 1494 1494  490  194  490 1361  167 1494  490\n",
      " 1361  167]\n",
      "Y_Sample: [0.62381357 0.4881904  0.36712185 0.6137994  0.27458775 0.00906727\n",
      " 0.06625423 0.33084142 0.4177143  0.00448501]\n",
      "Shape: (12235, 10)\n",
      "X_Sample: [4360   28   40 1470  307  560  199   37   43 3483   38  330  128  316\n",
      "   39  437  126  864   40   41   37  203  115  507  199   37  212  437\n",
      " 2357  128  316   77  203   41  175  871  437  154  530   67  867  836\n",
      " 1077   79  498  161  153  221  316   61  120  282  762 1892  280   79\n",
      "  175  871  437  154  530   67  867  836 1077   79  498  161  153  221\n",
      "  316   61  120  282  762 1892  280   79  175  871  437  154  530   67\n",
      "  867  836 1077   79  498  161  153  221  316   61  120  282  762 1892\n",
      "  280   79]\n",
      "Y_Sample: [0.08432251 0.66251063 0.41537276 0.5697108  0.8831378  0.05560896\n",
      " 0.12980586 0.8838371  0.50296223 0.38019416]\n",
      "Shape: (12235, 10)\n",
      "X_Sample: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0   143   685\n",
      "   685  1074   138   533   115   684   490    37   298    83    68 15628\n",
      "   319    11  1614   208  2433    72    38    68  1981    68  1981   138\n",
      "   533   115   684   490    37   298    83    68 15628   319    11  1614\n",
      "   208  2433    72    38    68  1981    68  1981    68  1981    68 15628\n",
      "   143   685   685  1074]\n",
      "Y_Sample: [0.18517691 0.7531231  0.60800916 0.5589222  0.7702304  0.00961503\n",
      " 0.09033811 0.7836337  0.48208278 0.01594168]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(\"Shape:\", Y_preds.shape)\n",
    "    print(\"X_Sample:\", pX_test[i])\n",
    "    print(\"Y_Sample:\", Y_preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('RNN_ver1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('myCNNmodel.h5')\n",
    "predict = model.predict_classes(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
